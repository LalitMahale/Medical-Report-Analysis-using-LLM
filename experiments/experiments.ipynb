{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65569206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source of report https://drlogy.com/blog/blood-report-format#:~:text=Blood%20Report%20Format%20Examples%20&%20PDF.%2010%20Key%20Blood%20Report\n",
    "\n",
    "# https://cdn1.lalpathlabs.com/live/reports/WM17S.pdf#:~:text=Test%20Report%20Test%20Name%20Results%20Units%20Bio.%20Ref.%20Interval%20Note"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6743d6e0",
   "metadata": {},
   "source": [
    "pip install easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6f95bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e1836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "ocr = easyocr.Reader(lang_list=['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "068c6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"temp_image/page_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b1e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ocr.readtext(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de9bcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DRLOGY PATHOLOGY LAB'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe455464",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [bbox[1] for bbox in result]\n",
    "\n",
    "text = \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48b07aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRLOGY PATHOLOGY LAB\n",
      "0123456789\n",
      "0912345678\n",
      "Accurate\n",
      "Caring\n",
      "Instant\n",
      "drlogypathlab@drlogy com\n",
      "105-108, SMART VISION COMPLEX, HEALTHCARE ROAD, OPPOSITE HEALTHCARE COMPLEX. MUMBAI\n",
      "689578\n",
      "WWW\n",
      "drlogy com\n",
      "Yash M. Patel\n",
      "Sample Collected At:\n",
      "21 Years\n",
      "125, Shivam Bungalow, S G Road,\n",
      "35545 62336 78\n",
      "Registered on: 02.31 PM 02 Dec, 2X\n",
      "Sex\n",
      "Male\n",
      "Mumbai\n",
      "Collected on: 03.11 PM 02\n",
      "2X\n",
      "PID\n",
      "555\n",
      "Ref. By: Dr. Hiren Shah\n",
      "Reported on: 04.35 PM 02 Dec, 2X\n",
      "Complete Blood Count (CBC)\n",
      "Investigation\n",
      "Result\n",
      "Reference Value\n",
      "Unit\n",
      "Primary Sample Type\n",
      "Blood\n",
      "HEMOGLOBIN\n",
      "Hemoglobin (Hb)\n",
      "12.5\n",
      "Low\n",
      "13.0 - 17.0\n",
      "g/dL\n",
      "RBC COUNT\n",
      "Total RBC count\n",
      "5.2\n",
      "4.5\n",
      "5.5\n",
      "mill/cumm\n",
      "BLOOD INDICES\n",
      "Packed Cell Volume (PCV)\n",
      "57.5\n",
      "High\n",
      "40\n",
      "50\n",
      "%\n",
      "Mean Corpuscular Volume (MCV)\n",
      "87.75\n",
      "83\n",
      "101\n",
      "fL\n",
      "Calculated\n",
      "MCH\n",
      "27.2\n",
      "27\n",
      "32\n",
      "pg\n",
      "Calculated\n",
      "MCHC\n",
      "32.8\n",
      "32.5 - 34.5\n",
      "g/dL\n",
      "Calculated\n",
      "RDW\n",
      "13.6\n",
      "11.6\n",
      "14.0\n",
      "%\n",
      "WBC COUNT\n",
      "Total WBC count\n",
      "\" J0oo\n",
      "4000-11000\n",
      "  Gm\n",
      "DIFFERENTIAL WBC COUNT\n",
      "Neutrophils\n",
      "60\n",
      "50\n",
      "62\n",
      "%\n",
      "Lymphocytes\n",
      "31\n",
      "\n",
      "20 - 40\n",
      "\n",
      "%\n",
      "Eosinophils\n",
      "1\n",
      "00\n",
      "06\n",
      "%\n",
      "Monocytes\n",
      "7\n",
      "00\n",
      "10\n",
      "%\n",
      "Basophils\n",
      "1\n",
      "00\n",
      "02\n",
      "%\n",
      "PLATELET COUNT\n",
      "Platelet Count\n",
      "150000\n",
      "Borderline\n",
      "150000\n",
      "410000\n",
      "cumm\n",
      "Instruments: Fully automated cell counter\n",
      "Mindray 300\n",
      "Interpretation: Further confirm for Anemia\n",
      "Thanks for Reference\n",
      "****End of Report****\n",
      "Medical Lab Technician\n",
      "Dr. Payal Shah\n",
      "Dr: Vimal Shah\n",
      "(DMLT, BMLT)\n",
      "(MD, Pathologist)\n",
      "(MD, Pathologist)\n",
      "Generated on\n",
      "02 Dec, 202X 05.00 PM\n",
      "Page 1 of 1\n",
      "Sample Collection\n",
      "0123456789\n",
      "Age\n",
      "Dec,\n",
      "78 0tz47\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc1020b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc5fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(model_name=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b935c00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Latest',\n",
      "      description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
      "                   'model.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
      "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
      "                   'model that supports tuning.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-1.5-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro Latest',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro 001',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Pro 002',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-exp-0801',\n",
      "      base_model_id='',\n",
      "      version='exp-0801',\n",
      "      display_name='Gemini 1.5 Pro Experimental 0801',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='exp-0827',\n",
      "      display_name='Gemini 1.5 Pro Experimental 0827',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash Latest',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 001',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-001-tuning',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=16384,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='exp-0827',\n",
      "      display_name='Gemini 1.5 Flash Experimental 0827',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Flash 002',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B 001',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B Latest',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/text-embedding-004',\n",
      "      base_model_id='',\n",
      "      version='004',\n",
      "      display_name='Text Embedding 004',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "for i in genai.list_models():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f5c527a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_content(\"hi\").text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c7d4150",
   "metadata": {},
   "source": [
    "pip install Pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e4ce518",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\Desktop\\LLMProject\\LLM_projects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7168cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document import PDF_Processing\n",
    "from src.pipeline import LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a039fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\Desktop\\LLMProject\\LLM_projects\\medical_report_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd medical_report_analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43204c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"test_images/WM17S.pdf\"\n",
    "image = PDF_Processing.pdf_to_image(file=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8564af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt=\"\"\"\n",
    "You are skilled in interpreting images, specifically medical reports. Your task is to extract all the text from the provided images, capturing every detail exactly as it appears\n",
    "\"\"\"\n",
    "file = r\"C:\\Users\\lalit\\Downloads\\4a79b38b-5cb2-4c87-b4f9-7c18fd615711.jpg\"\n",
    "# image = Image.open(file)\n",
    "query =  \"extract detail and number as given in image\"\n",
    "#\"\"\"\n",
    "# Give output in give json format : \\n\n",
    "# {\n",
    "#   \"patient_name\": \"<patient name>\",\n",
    "#   \"lab_no\": \"<lab number>\",\n",
    "#   \"lab_name\": \"<laboratory name>\",\n",
    "#   \"collection_date_time\": \"<sample collection date and time>\",\n",
    "#   \"reported_date_time\": \"<report date and time>\",\n",
    "#   \"patient_age\": \"<patient age>\",\n",
    "#   \"patient_gender\": \"<patient gender>\",\n",
    "#   \"Report\": {\n",
    "#     \"<investigation name>\": {\n",
    "#       \"result\": \"<result value>\",\n",
    "#       \"reference_value\": \"<reference range>\",\n",
    "#       \"unit\": \"<unit of measurement>\"\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "res = LLM().gemini_vision_llm([query,image,input_prompt])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e1e1da7",
   "metadata": {},
   "source": [
    "PDF_Processing.get_clean_json(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3b036528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                : DUMMY\n",
      "Lab No.             : 439854467\n",
      "Ref By              : U\n",
      "Collected         : 14/5/2023 11:03:00AM\n",
      "A/c Status         : P\n",
      "Collected at      : LPL-ROHINI (NATIONAL REFERENCE LAB)\n",
      "                             National Reference laboratory, Block E, Sector\n",
      "                             18, ROHINI\n",
      "                             DELHI 110085\n",
      "Age                 : 30 Years\n",
      "Gender              : Male\n",
      "Reported          : 16/5/2023 1:36:25PM\n",
      "Report Status     : Final\n",
      "Processed at     : LPL-NATIONAL REFERENCE LAB\n",
      "                              National Reference laboratory, Block E,\n",
      "                             Sector 18, Rohini, New Delhi -110085\n",
      "\n",
      "Test Name                                              Results           Units               Bio. Ref. Interval \n",
      "SwasthFit Super 4\n",
      "\n",
      "COMPLETE BLOOD COUNT;CBC\n",
      "Hemoglobin                                               15.00               g/dL                      13.00 - 17.00\n",
      "(Photometry)\n",
      "Packed Cell Volume (PCV)                         45.00               %                            40.00 - 50.00\n",
      "(Calculated)\n",
      "RBC Count                                              4.50                mill/mm3                4.50 - 5.50\n",
      "(Electrical Impedence)\n",
      "MCV                                                      90.00              fL                            83.00 - 101.00\n",
      "(Electrical Impedence)\n",
      "MCH                                                      32.00               pg                           27.00 - 32.00\n",
      "(Calculated)\n",
      "MCHC                                                     33.00               g/dL                      31.50 - 34.50\n",
      "(Calculated)\n",
      "Red Cell Distribution Width (RDW)                 14.00               %                            11.60 - 14.00\n",
      "(Electrical Impedence)\n",
      "Total Leukocyte Count (TLC)                      8.00               thou/mm3               4.00 - 10.00\n",
      "(Electrical Impedence)\n",
      "\n",
      "Differential Leucocyte Count (DLC)\n",
      "(VCS Technology)\n",
      "Segmented Neutrophils                           60.00               %                            40.00 - 80.00\n",
      "Lymphocytes                                             30.00               %                            20.00 - 40.00\n",
      "Monocytes                                                5.00                %                            2.00 - 10.00\n",
      "Eosinophils                                                5.00                %                            1.00 - 6.00\n",
      "Basophils                                                  0.00                %                            <2.00\n",
      "\n",
      "Absolute Leucocyte Count \n",
      "(Calculated)\n",
      "Neutrophils                                              4.80                 thou/mm3                2.00 - 7.00\n",
      "Lymphocytes                                             2.40                 thou/mm3                1.00 - 3.00\n",
      "Monocytes                                                0.40                 thou/mm3                0.20 - 1.00\n",
      "Eosinophils                                                0.40                 thou/mm3                0.02 - 0.50\n",
      "Basophils                                                  0.00                 thou/mm3                0.02 - 0.10\n",
      "\n",
      "Platelet Count                                          200                  thou/mm3               150.00 - 410.00\n",
      "(Electrical Impedence)\n",
      "Mean Platelet Volume                            10.0                  fL                             6.5 - 12.0\n",
      "(Electrical Impedence)\n",
      "\n",
      "Page 1 of 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9c71ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are an expert in text comprehension. Your task is straightforward: understand the provided text and return the output in the specified JSON format.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Output: Please provide the output in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"patient_name\": \"<patient name>\",\n",
    "  \"lab_no\": \"<lab number>\",\n",
    "  \"lab_name\": \"<laboratory name>\",\n",
    "  \"collection_date_time\": \"<sample collection date and time>\",\n",
    "  \"reported_date_time\": \"<report date and time>\"\n",
    "  \"test_name\": \"<test name>\",\n",
    "  \"patient_age\": \"<patient age>\",\n",
    "  \"patient_gender\": \"<patient gender>\",\n",
    "  \"Report\": {{\n",
    "    \"<investigation name>\": {{\n",
    "      \"result\": \"<result value>\",\n",
    "      \"reference_value\": \"<reference range>\",\n",
    "      \"unit\": \"<unit of measurement>\"\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cc8e0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.generate_content(prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eac974b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"patient_name\": \"Yash M. Patel\",\n",
      "  \"lab_no\": \"689578\",\n",
      "  \"lab_name\": \"DRLOGY PATHOLOGY LAB\",\n",
      "  \"collection_date_time\": \"03.11 PM 02 Dec, 2X\",\n",
      "  \"reported_date_time\": \"04.35 PM 02 Dec, 2X\",\n",
      "  \"test_name\": \"Complete Blood Count (CBC)\",\n",
      "  \"patient_age\": \"21 Years\",\n",
      "  \"patient_gender\": \"Male\",\n",
      "  \"Report\": {\n",
      "    \"HEMOGLOBIN\": {\n",
      "      \"result\": \"12.5\",\n",
      "      \"reference_value\": \"13.0 - 17.0\",\n",
      "      \"unit\": \"g/dL\"\n",
      "    },\n",
      "    \"RBC COUNT\": {\n",
      "      \"result\": \"5.2\",\n",
      "      \"reference_value\": \"4.5 - 5.5\",\n",
      "      \"unit\": \"mill/cumm\"\n",
      "    },\n",
      "    \"BLOOD INDICES\": {\n",
      "      \"Packed Cell Volume (PCV)\": {\n",
      "        \"result\": \"57.5\",\n",
      "        \"reference_value\": \"40 - 50\",\n",
      "        \"unit\": \"%\"\n",
      "      },\n",
      "      \"Mean Corpuscular Volume (MCV)\": {\n",
      "        \"result\": \"87.75\",\n",
      "        \"reference_value\": \"83 - 101\",\n",
      "        \"unit\": \"fL\"\n",
      "      },\n",
      "      \"MCH\": {\n",
      "        \"result\": \"27.2\",\n",
      "        \"reference_value\": \"27 - 32\",\n",
      "        \"unit\": \"pg\"\n",
      "      },\n",
      "      \"MCHC\": {\n",
      "        \"result\": \"32.8\",\n",
      "        \"reference_value\": \"32.5 - 34.5\",\n",
      "        \"unit\": \"g/dL\"\n",
      "      },\n",
      "      \"RDW\": {\n",
      "        \"result\": \"13.6\",\n",
      "        \"reference_value\": \"11.6 - 14.0\",\n",
      "        \"unit\": \"%\"\n",
      "      }\n",
      "    },\n",
      "    \"WBC COUNT\": {\n",
      "      \"result\": \"10000\",\n",
      "      \"reference_value\": \"4000-11000\",\n",
      "      \"unit\": \"  Gm\"\n",
      "    },\n",
      "    \"DIFFERENTIAL WBC COUNT\": {\n",
      "      \"Neutrophils\": {\n",
      "        \"result\": \"60\",\n",
      "        \"reference_value\": \"50 - 62\",\n",
      "        \"unit\": \"%\"\n",
      "      },\n",
      "      \"Lymphocytes\": {\n",
      "        \"result\": \"31\",\n",
      "        \"reference_value\": \"20 - 40\",\n",
      "        \"unit\": \"%\"\n",
      "      },\n",
      "      \"Eosinophils\": {\n",
      "        \"result\": \"1\",\n",
      "        \"reference_value\": \"00 - 06\",\n",
      "        \"unit\": \"%\"\n",
      "      },\n",
      "      \"Monocytes\": {\n",
      "        \"result\": \"7\",\n",
      "        \"reference_value\": \"00 - 10\",\n",
      "        \"unit\": \"%\"\n",
      "      },\n",
      "      \"Basophils\": {\n",
      "        \"result\": \"1\",\n",
      "        \"reference_value\": \"00 - 02\",\n",
      "        \"unit\": \"%\"\n",
      "      }\n",
      "    },\n",
      "    \"PLATELET COUNT\": {\n",
      "      \"result\": \"150000\",\n",
      "      \"reference_value\": \"150000 - 410000\",\n",
      "      \"unit\": \"cumm\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc38965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "02ae9e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_name': 'Yash M. Patel',\n",
       " 'lab_no': '689578',\n",
       " 'lab_name': 'DRLOGY PATHOLOGY LAB',\n",
       " 'collection_date_time': '03.11 PM 02 Dec, 2X',\n",
       " 'reported_date_time': '04.35 PM 02 Dec, 2X',\n",
       " 'test_name': 'Complete Blood Count (CBC)',\n",
       " 'patient_age': '21 Years',\n",
       " 'patient_gender': 'Male',\n",
       " 'Report': {'HEMOGLOBIN': {'result': '12.5',\n",
       "   'reference_value': '13.0 - 17.0',\n",
       "   'unit': 'g/dL'},\n",
       "  'RBC COUNT': {'result': '5.2',\n",
       "   'reference_value': '4.5 - 5.5',\n",
       "   'unit': 'mill/cumm'},\n",
       "  'BLOOD INDICES': {'Packed Cell Volume (PCV)': {'result': '57.5',\n",
       "    'reference_value': '40 - 50',\n",
       "    'unit': '%'},\n",
       "   'Mean Corpuscular Volume (MCV)': {'result': '87.75',\n",
       "    'reference_value': '83 - 101',\n",
       "    'unit': 'fL'},\n",
       "   'MCH': {'result': '27.2', 'reference_value': '27 - 32', 'unit': 'pg'},\n",
       "   'MCHC': {'result': '32.8',\n",
       "    'reference_value': '32.5 - 34.5',\n",
       "    'unit': 'g/dL'},\n",
       "   'RDW': {'result': '13.6', 'reference_value': '11.6 - 14.0', 'unit': '%'}},\n",
       "  'WBC COUNT': {'result': '10000',\n",
       "   'reference_value': '4000-11000',\n",
       "   'unit': '  Gm'},\n",
       "  'DIFFERENTIAL WBC COUNT': {'Neutrophils': {'result': '60',\n",
       "    'reference_value': '50 - 62',\n",
       "    'unit': '%'},\n",
       "   'Lymphocytes': {'result': '31', 'reference_value': '20 - 40', 'unit': '%'},\n",
       "   'Eosinophils': {'result': '1', 'reference_value': '00 - 06', 'unit': '%'},\n",
       "   'Monocytes': {'result': '7', 'reference_value': '00 - 10', 'unit': '%'},\n",
       "   'Basophils': {'result': '1', 'reference_value': '00 - 02', 'unit': '%'}},\n",
       "  'PLATELET COUNT': {'result': '150000',\n",
       "   'reference_value': '150000 - 410000',\n",
       "   'unit': 'cumm'}}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b166de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image is a blood test report from Dr Lal PathLabs for a patient named \"DUMMY\". It shows the results of a \"SwasthFit Super 4\" test, which includes a complete blood count (CBC). \\n\\nThe report includes details such as:\\n\\n* **Patient Information:** Name, Lab No., Age, Gender\\n* **Sample Information:**  Collected date & time, Report date & time,  A/c Status\\n* **Test Results:** Detailed results for various blood components like Hemoglobin, RBC Count, WBC Count, Platelet count and more.  Each result is presented with the actual value, unit of measurement, and the biological reference interval.\\n\\nThe report also includes contact information for customer care in case of any alarming or unexpected results. \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a68c64fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\Desktop\\LLMProject\\LLM_projects\\medical_report_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94b9a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a variety of South Indian breakfast dishes served on banana leaves. The dishes include:\n",
      "\n",
      "* **Dosa:** A thin, crispy crepe made from fermented rice and lentil batter. There are two types of dosa in the image: a plain dosa and a masala dosa (filled with a spiced potato mixture).\n",
      "* **Idli:** Steamed rice cakes served with sambar (a lentil-based vegetable stew) and chutney (coconut chutney).\n",
      "* **Parotta:** Layered flatbread served with a curry (possibly kurma, a creamy coconut-based gravy).\n",
      "\n",
      "Each dish is accompanied by small bowls of sambar and chutney. The picture seems to be taken at a restaurant called \"Tiffin Times\". \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a469304d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\lalit\\\\Desktop\\\\LLMProject\\\\LLM_projects\\\\medical_report_analysis\\\\experiments'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214c059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\Desktop\\LLMProject\\LLM_projects\\medical_report_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdb181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document import PDF_Processing\n",
    "from src.ocr_model import OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547ae0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\Desktop\\LLMProject\\LLM_projects\\medical_report_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a66c454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "image = PDF_Processing.pdf_to_image(\"test_docs/CBC-test-report-format-example-sample-template-Drlogy-lab-report.pdf\")\n",
    "text = OCR.extract_text(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09eafa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Dr Lal PathLala\n",
      "Regd: Office/National Reference Lab: Dr Lal PathLabs Ltd, Block-E, Sector-18, Rohini, New Delhi-1 10085\n",
      "Wcb: www lalpathlabs com; CIN No:: L74899DL1995PL.C065388\n",
      "Name\n",
      "DUMMY\n",
      "\n",
      "Lab No_\n",
      "439854467\n",
      "Age\n",
      "30 Years\n",
      "\n",
      "Ref By\n",
      "U\n",
      "Gender\n",
      "Male\n",
      "\n",
      "MC-2113\n",
      "Collected\n",
      "14/5/2023  11:03:OOAM\n",
      "Reported\n",
      "16/5/2023  1:36.25PM\n",
      "Alc Status\n",
      "P\n",
      "Report Status\n",
      "Final\n",
      "Collected at\n",
      "LPL-ROHINI (NATIONAL REFERENCE LAB)\n",
      "Processed at\n",
      "LPL-NATIONAL REFERENCE LAB\n",
      "National Reference laboratory, Block E, Sector\n",
      "National Reference laboratory, Block E,\n",
      "18, ROHINI\n",
      "Sector 18, Rohini; New Delhi -110085\n",
      "DELHI 110085\n",
      "Test Report\n",
      "Test Name\n",
      "Results\n",
      "Units\n",
      "Bio. Ref. Interval\n",
      "SwasthFit Super 4\n",
      "COMPLETE BLOOD COUNT;CBC\n",
      "Hemoglobin\n",
      "15.00\n",
      "13.00\n",
      "17.00\n",
      "(Photometry)\n",
      "Packed Cell Volume (PCV)\n",
      "45.00\n",
      "%\n",
      "40.00\n",
      "50.00\n",
      "(Calculated)\n",
      "RBC Count\n",
      "4.50\n",
      "milllmm3\n",
      "4.50\n",
      "5.50\n",
      "(Electrical Impedence)\n",
      "MCV\n",
      "90.00\n",
      "fL\n",
      "83.00\n",
      "101.00\n",
      "(Electrical Impedence)\n",
      "MCH\n",
      "32.00\n",
      "pg\n",
      "27.00\n",
      "32.00\n",
      "(Calculated)\n",
      "MCHC\n",
      "33.00\n",
      "31.50\n",
      "34.50\n",
      "(Calculated)\n",
      "Red Cell Distribution Width (RDW)\n",
      "14.00\n",
      "11.60\n",
      "14.00\n",
      "(Electrical Impedence)\n",
      "Total Leukocyte Count (TLC)\n",
      "8.00\n",
      "thoulmm3\n",
      "4.00\n",
      "10.00\n",
      "(Electrical Impedence)\n",
      "Differential Leucocyte Count (DLC)\n",
      "(VCS Technology)\n",
      "Segmented Neutrophils\n",
      "60.00\n",
      "%\n",
      "40.00\n",
      "80.00\n",
      "Lymphocytes\n",
      "30.00\n",
      "%\n",
      "20.00\n",
      "40.00\n",
      "Monocytes\n",
      "5.00\n",
      "%\n",
      "2.00\n",
      "10.00\n",
      "Eosinophils\n",
      "5.00\n",
      "%\n",
      "1.00\n",
      "6.00\n",
      "Basophils\n",
      "0.00\n",
      "%\n",
      "<2.00\n",
      "Absolute Leucocyte Count\n",
      "(Calculated)\n",
      "Neutrophils\n",
      "4.80\n",
      "thoulmm3\n",
      "2.00\n",
      "7.00\n",
      "Lymphocytes\n",
      "2.40\n",
      "thoulmm3\n",
      "1.00\n",
      "3.00\n",
      "Monocytes\n",
      "0.40\n",
      "thoulmm3\n",
      "0.20\n",
      "1.00\n",
      "Eosinophils\n",
      "0.40\n",
      "thoulmm3\n",
      "0.02\n",
      "0.50\n",
      "Basophils\n",
      "0.00\n",
      "thoulmm3\n",
      "0.02\n",
      "0.10\n",
      "Platelet Count\n",
      "200\n",
      "thoulmm3\n",
      "150.00\n",
      "410.00\n",
      "(Electrical impedence)\n",
      "Mean Platelet Volume\n",
      "10.0\n",
      "6.5\n",
      "12.0\n",
      "(Electrical Impedence)\n",
      "Page 1 of 10\n",
      "If Test results are\n",
      "alarming or unexpected,client is advised to contact the Customer Care immediately for possible remedial action:\n",
      "Tel: +91-11-3988-5050,E-mail: lalpathlabs@lalpathlabs.com\n",
      "gldL\n",
      "gldL\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ae2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are an expert in text comprehension. Your task is straightforward: understand the provided text and return the output in the specified JSON format.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Output: Please provide the output in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"patient_name\": \"<patient name>\",\n",
    "  \"lab_no\": \"<lab number>\",\n",
    "  \"lab_name\": \"<laboratory name>\",\n",
    "  \"collection_date_time\": \"<sample collection date and time>\",\n",
    "  \"reported_date_time\": \"<report date and time>\"\n",
    "  \"test_name\": \"<test name>\",\n",
    "  \"patient_age\": \"<patient age>\",\n",
    "  \"patient_gender\": \"<patient gender>\",\n",
    "  \"Report\": {{\n",
    "    \"<investigation name>\": {{\n",
    "      \"result\": \"<result value>\",\n",
    "      \"reference_value\": \"<reference range>\",\n",
    "      \"unit\": \"<unit of measurement>\"\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c5ea4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"patient_name\": \"DUMMY\",\n",
      "  \"lab_no\": \"439854467\",\n",
      "  \"lab_name\": \"Dr Lal PathLabs Ltd\",\n",
      "  \"collection_date_time\": \"14/5/2023  11:03:OOAM\",\n",
      "  \"reported_date_time\": \"16/5/2023  1:36.25PM\",\n",
      "  \"test_name\": \"SwasthFit Super 4\",\n",
      "  \"patient_age\": \"30 Years\",\n",
      "  \"patient_gender\": \"Male\",\n",
      "  \"Report\": {\n",
      "    \"COMPLETE BLOOD COUNT;CBC\": {\n",
      "      \"Hemoglobin\": {\n",
      "        \"result\": \"15.00\",\n",
      "        \"reference_value\": \"13.00 - 17.00\",\n",
      "        \"unit\": \"gldL\"\n",
      "      },\n",
      "      \"Packed Cell Volume (PCV)\": {\n",
      "        \"result\": \"45.00\",\n",
      "        \"reference_value\": \"40.00 - 50.00\",\n",
      "        \"unit\": \"%\"\n",
      "      },\n",
      "      \"RBC Count\": {\n",
      "        \"result\": \"4.50\",\n",
      "        \"reference_value\": \"4.50 - 5.50\",\n",
      "        \"unit\": \"milllmm3\"\n",
      "      },\n",
      "      \"MCV\": {\n",
      "        \"result\": \"90.00\",\n",
      "        \"reference_value\": \"83.00 - 101.00\",\n",
      "        \"unit\": \"fL\"\n",
      "      },\n",
      "      \"MCH\": {\n",
      "        \"result\": \"32.00\",\n",
      "        \"reference_value\": \"27.00 - 32.00\",\n",
      "        \"unit\": \"pg\"\n",
      "      },\n",
      "      \"MCHC\": {\n",
      "        \"result\": \"33.00\",\n",
      "        \"reference_value\": \"31.50 - 34.50\",\n",
      "        \"unit\": null\n",
      "      },\n",
      "      \"Red Cell Distribution Width (RDW)\": {\n",
      "        \"result\": \"14.00\",\n",
      "        \"reference_value\": \"11.60 - 14.00\",\n",
      "        \"unit\": null\n",
      "      },\n",
      "      \"Total Leukocyte Count (TLC)\": {\n",
      "        \"result\": \"8.00\",\n",
      "        \"reference_value\": \"4.00 - 10.00\",\n",
      "        \"unit\": \"thoulmm3\"\n",
      "      },\n",
      "      \"Differential Leucocyte Count (DLC)\": {\n",
      "        \"Segmented Neutrophils\": {\n",
      "          \"result\": \"60.00\",\n",
      "          \"reference_value\": \"40.00 - 80.00\",\n",
      "          \"unit\": \"%\"\n",
      "        },\n",
      "        \"Lymphocytes\": {\n",
      "          \"result\": \"30.00\",\n",
      "          \"reference_value\": \"20.00 - 40.00\",\n",
      "          \"unit\": \"%\"\n",
      "        },\n",
      "        \"Monocytes\": {\n",
      "          \"result\": \"5.00\",\n",
      "          \"reference_value\": \"2.00 - 10.00\",\n",
      "          \"unit\": \"%\"\n",
      "        },\n",
      "        \"Eosinophils\": {\n",
      "          \"result\": \"5.00\",\n",
      "          \"reference_value\": \"1.00 - 6.00\",\n",
      "          \"unit\": \"%\"\n",
      "        },\n",
      "        \"Basophils\": {\n",
      "          \"result\": \"0.00\",\n",
      "          \"reference_value\": \"<2.00\",\n",
      "          \"unit\": \"%\"\n",
      "        }\n",
      "      },\n",
      "      \"Absolute Leucocyte Count\\n(Calculated)\": {\n",
      "        \"Neutrophils\": {\n",
      "          \"result\": \"4.80\",\n",
      "          \"reference_value\": \"2.00 - 7.00\",\n",
      "          \"unit\": \"thoulmm3\"\n",
      "        },\n",
      "        \"Lymphocytes\": {\n",
      "          \"result\": \"2.40\",\n",
      "          \"reference_value\": \"1.00 - 3.00\",\n",
      "          \"unit\": \"thoulmm3\"\n",
      "        },\n",
      "        \"Monocytes\": {\n",
      "          \"result\": \"0.40\",\n",
      "          \"reference_value\": \"0.20 - 1.00\",\n",
      "          \"unit\": \"thoulmm3\"\n",
      "        },\n",
      "        \"Eosinophils\": {\n",
      "          \"result\": \"0.40\",\n",
      "          \"reference_value\": \"0.02 - 0.50\",\n",
      "          \"unit\": \"thoulmm3\"\n",
      "        },\n",
      "        \"Basophils\": {\n",
      "          \"result\": \"0.00\",\n",
      "          \"reference_value\": \"0.02 - 0.10\",\n",
      "          \"unit\": \"thoulmm3\"\n",
      "        }\n",
      "      },\n",
      "      \"Platelet Count\": {\n",
      "        \"result\": \"200\",\n",
      "        \"reference_value\": \"150.00 - 410.00\",\n",
      "        \"unit\": \"thoulmm3\"\n",
      "      },\n",
      "      \"Mean Platelet Volume\": {\n",
      "        \"result\": \"10.0\",\n",
      "        \"reference_value\": \"6.5 - 12.0\",\n",
      "        \"unit\": null\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = model.generate_content(prompt).text\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f6f053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (2.4.1)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "Using cached huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 26.3 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.25.1 safetensors-0.4.5 tokenizers-0.20.0 transformers-4.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch huggingface-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da05ede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\lalit\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# Replace 'your_token' with your Hugging Face token\n",
    "login(os.getenv(\"HUGGING_FACE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|████████████████████| 8/8 [05:15<00:00, 39.44s/it]\n",
      "Loading checkpoint shards:   0%|                     | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model using the pipeline\n",
    "model_name = \"epfl-llm/meditron-7b\"\n",
    "text_generator = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "# Generate text based on a prompt\n",
    "prompt = \"What are the common symptoms of diabetes?\"\n",
    "output = text_generator(prompt, max_length=100)\n",
    "\n",
    "print(output[0]['generated_text'])  # Output the generated text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd8eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = text_generator(\"how to know i have diabetes.\", max_length=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe03a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\lalit\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lalit\\.cache\\huggingface\\hub\\models--HuggingFaceH4--zephyr-7b-beta. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|████████████████████| 8/8 [10:26<00:00, 78.35s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4482a38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from langchain-huggingface) (0.25.1)\n",
      "Collecting langchain-core<0.4,>=0.3.0 (from langchain-huggingface)\n",
      "  Downloading langchain_core-0.3.9-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from langchain-huggingface) (0.20.0)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from langchain-huggingface) (4.45.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4,>=0.3.0->langchain-huggingface)\n",
      "  Downloading langsmith-0.1.131-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.4.1)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.14.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.10.7)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.8.30)\n",
      "Requirement already satisfied: sympy in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lalit\\anaconda3\\envs\\gen_ai\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading langchain_core-0.3.9-py3-none-any.whl (401 kB)\n",
      "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "Downloading langsmith-0.1.131-py3-none-any.whl (294 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.4/11.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 14.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 16.3 MB/s eta 0:00:00\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn, requests-toolbelt, langsmith, langchain-core, sentence-transformers, langchain-huggingface\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.108\n",
      "    Uninstalling langsmith-0.1.108:\n",
      "      Successfully uninstalled langsmith-0.1.108\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.37\n",
      "    Uninstalling langchain-core-0.2.37:\n",
      "      Successfully uninstalled langchain-core-0.2.37\n",
      "Successfully installed langchain-core-0.3.9 langchain-huggingface-0.1.0 langsmith-0.1.131 requests-toolbelt-1.0.0 scikit-learn-1.5.2 sentence-transformers-3.1.1 threadpoolctl-3.5.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.2.15 requires langchain-core<0.3.0,>=0.2.35, but you have langchain-core 0.3.9 which is incompatible.\n",
      "langchain-community 0.2.15 requires langchain-core<0.3.0,>=0.2.37, but you have langchain-core 0.3.9 which is incompatible.\n",
      "langchain-google-genai 1.0.10 requires langchain-core<0.3,>=0.2.33, but you have langchain-core 0.3.9 which is incompatible.\n",
      "langchain-text-splitters 0.2.2 requires langchain-core<0.3.0,>=0.2.10, but you have langchain-core 0.3.9 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa89c95a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pillow\n",
      "Version: 10.4.0\n",
      "Summary: Python Imaging Library (Fork)\n",
      "Home-page: https://python-pillow.org\n",
      "Author: \n",
      "Author-email: \"Jeffrey A. Clark\" <aclark@aclark.net>\n",
      "License: HPND\n",
      "Location: C:\\Users\\lalit\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\n",
      "Requires: \n",
      "Required-by: easyocr, imageio, llama-index-core, scikit-image, sentence-transformers, streamlit, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174bc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
